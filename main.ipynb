{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom ChatBot using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to develop a chatbot utilizing the Retrieval-Augmented Generation (RAG) method, leveraging a custom dataset built from a PDF containing my CV information. The chatbot will be designed to answer questions specifically related to my educational background, providing accurate and context-aware responses. This approach integrates document retrieval with language generation to enhance the chatbot’s ability to understand and deliver relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_file = r\"C:\\Users\\alsot\\Downloads\\CV_TAUA.pdf\"\n",
    "\n",
    "# Open the PDF file\n",
    "with open(pdf_file, \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Extract text from each page\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "# Reconstruct paragraphs\n",
    "paragraphs = []\n",
    "current_paragraph = \"\"\n",
    "\n",
    "# Split the text into lines\n",
    "lines = text.replace(\"\\n\",\"\").split(\".\")\n",
    "\n",
    "lines_trim = [line.strip() for line in lines]\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(lines, columns=[\"Paragraph\"])\n",
    "df = df[~df[\"Paragraph\"].str.strip().eq(\"\")].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load credentials and create embeddings from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "openai.api_base = config[\"api_base\"]\n",
    "openai.api_key = config[\"api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    # Send text data to OpenAI model to get embeddings\n",
    "    response = openai.Embedding.create(\n",
    "        input=df.iloc[i:i+batch_size][\"Paragraph\"].tolist(),\n",
    "        engine=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # Add embeddings to list\n",
    "    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "# Add embeddings list to dataframe\n",
    "df[\"embeddings\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the cosine distance between embeddings from CV and the created question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "    \"\"\"\n",
    "    Function that takes in a question string and a dataframe containing\n",
    "    rows of text and associated embeddings, and returns that dataframe\n",
    "    sorted from least to most relevant for that question\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get embeddings for the question text\n",
    "    question_embeddings = get_embedding(question, engine=EMBEDDING_MODEL_NAME)\n",
    "    \n",
    "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "    # the cosine distances between each row's embeddings and the\n",
    "    # embeddings of the question\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        question_embeddings,\n",
    "        df_copy[\"embeddings\"].values,\n",
    "        distance_metric=\"cosine\"\n",
    "    )\n",
    "    \n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    # (shorter distance = more relevant so we sort in ascending order)\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rows_sorted_by_relevance(\"Who is Tauã?\", df).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing the Previously Created Function to Retrieve the Most Relevant Context from Stored Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context:  :\n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "    \n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"Paragraph\"].values:\n",
    "        \n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "        \n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_prompt(\"who is Taua?\", df, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Retrieve Answers from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=3000, max_answer_tokens=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "    \n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = create_prompt(question, df, max_prompt_tokens)\n",
    "    \n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens,\n",
    "            temperature = 0\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Question: who is Taua\n",
      "\n",
      "    Original Answer: As an AI, I do not have access to personal information such as a person's name. Can you provide more context or details about Taua?\n",
      "    Custom Answer:   Tauã is a Product Analyst at Itaú with a background in Mechanical Engineering and expertise in data analytics, process automation, and machine learning applications. He is fluent in English and holds certifications in Analytics Engineering Associate, Generative AI Associate, and Data Science Associate from Itaú. He has experience with internal bank databases and has pursued additional education in business. He is currently pursuing the Gen IA Nanodegree by Udacity to deepen his knowledge in artificial intelligence applications. He is committed to refining his\n",
      "    \n",
      "\n",
      "    Question: where Taua graduated?\n",
      "\n",
      "    Original Answer: I'm afraid I can't answer that question as there is no information given about Taua or when they graduated. Do you have any other information that could help me answer your question?\n",
      "    Custom Answer:   Tauã graduated from the Polytechnic School of the University of São Paulo.\n",
      "    \n",
      "Exiting.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"Type your question (or 'exit' to exit): \").strip()\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        initial_who_is_answer = openai.Completion.create(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            prompt=f\"Question: {user_input}\\nAnswer:\",\n",
    "            max_tokens=150\n",
    "        )[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        initial_who_is_answer = f\"Error: {e}\"\n",
    "\n",
    "    custom_who_is_answer = answer_question(user_input, df)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Question: {user_input}\n",
    "\n",
    "    Original Answer: {initial_who_is_answer}\n",
    "    Custom Answer:   {custom_who_is_answer}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the customized model demonstrates excellent performance by accurately answering both questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
